{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4dc4f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f770ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 1 4 4 1 4 2 3 6 3]\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "3    0.178\n",
      "4    0.173\n",
      "2    0.171\n",
      "1    0.165\n",
      "6    0.158\n",
      "5    0.155\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "['Tails' 'Tails' 'Heads' 'Tails' 'Heads' 'Tails' 'Heads' 'Heads' 'Tails'\n",
      " 'Heads']\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "Heads    0.518\n",
      "Tails    0.482\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "{'Heads': 0.5, 'Tails': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Q.1 - Simulate a fair dice roll 1000 times using numpy.random.randint().\n",
    "\n",
    "dice_rolls = np.random.randint(1, 7, size=1000)\n",
    "print(dice_rolls[:10])\n",
    "print(\"\\n-----------------------------------------------------\\n\")\n",
    "\n",
    "# - Calculate the probability of each outcome using frequency count.\n",
    "\n",
    "outcome_counts = pd.Series(dice_rolls).value_counts(normalize=True)\n",
    "print(outcome_counts)\n",
    "print(\"\\n-----------------------------------------------------\\n\")\n",
    "\n",
    "# - Simulate a coin toss 500 times.\n",
    "\n",
    "coin_tosses = np.random.choice(['Heads', 'Tails'], size=500)\n",
    "print(coin_tosses[:10])\n",
    "print(\"\\n-----------------------------------------------------\\n\")\n",
    "\n",
    "# - Estimate the probability of getting heads and tails.\n",
    "\n",
    "coin_outcome_counts = pd.Series(coin_tosses).value_counts(normalize=True)\n",
    "print(coin_outcome_counts)\n",
    "print(\"\\n-----------------------------------------------------\\n\")\n",
    "\n",
    "# - Use collections. Counter or value_counts() to calculate experimental probability.\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "coin_tosses = ['Heads', 'Tails', 'Heads', 'Heads', 'Tails', 'Tails', 'Heads', 'Tails', 'Heads', 'Tails']\n",
    "counts = Counter(coin_tosses)\n",
    "total = sum(counts.values())\n",
    "probabilities = {outcome: count / total for outcome, count in counts.items()}\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ed760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of data1: 50.096660279111624, Mean of data2: 52.35418118624578\n",
      "T-statistic: -10.21466312215938, P-value: 6.522675122877795e-24\n"
     ]
    }
   ],
   "source": [
    "# Q.2 - Simulate two sets of data with slightly different means.\n",
    "\n",
    "np.random.seed(42)\n",
    "data1 = np.random.normal(loc=50, scale=5, size=1000)\n",
    "data2 = np.random.normal(loc=52, scale=5, size=1000)\n",
    "data1.mean(), data2.mean()\n",
    "print(f\"Mean of data1: {data1.mean()}, Mean of data2: {data2.mean()}\")\n",
    "\n",
    "# - Perform a t-test using scipy.stats.ttest_ind()) to get the p-value.\n",
    "\n",
    "t_stat, p_value = stats.ttest_ind(data1, data2)\n",
    "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8e9c7c",
   "metadata": {},
   "source": [
    "# - Interpret p-value in markdown. Mention what it implies for the null hypothesis.\n",
    "The p-value is the probability of obtaining results at least as extreme as the observed results, assuming that the null hypothesis is true.\n",
    "\n",
    "| **p-value** | **Interpretation**                                                                      |\n",
    "| ----------- | --------------------------------------------------------------------------------------- |\n",
    "| `p > 0.05`  | Weak evidence **against** the null hypothesis → **Fail to reject** the null hypothesis. |\n",
    "| `p ≤ 0.05`  | Strong evidence **against** the null hypothesis → **Reject** the null hypothesis.       |\n",
    "| `p ≤ 0.01`  | Very strong evidence **against** the null hypothesis → **Reject** with high confidence. |\n",
    "| `p ≤ 0.001` | Extremely strong evidence **against** the null → Highly significant result.             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cfc8e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paired T-statistic: -2.3453508116893143, P-value: 0.026060803515081003\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "Mean: 76.03834766892923\n",
      "95% Confidence Interval: (np.float64(72.33972084030087), np.float64(79.7369744975576))\n",
      "Reject the null hypothesis: There is a significant difference in scores before and after the training program.\n"
     ]
    }
   ],
   "source": [
    "# Q.3 - Create a sample dataset with scores before and after a training program.\n",
    "\n",
    "before_scores = np.random.normal(loc=70, scale=10, size=30)\n",
    "after_scores = np.random.normal(loc=75, scale=10, size=30)\n",
    "\n",
    "# - Use a paired t-test or an independent t-test to check for improvement.\n",
    "\n",
    "t_stat, p_value = stats.ttest_rel(before_scores, after_scores)\n",
    "print(f\"Paired T-statistic: {t_stat}, P-value: {p_value}\")\n",
    "print(\"\\n-----------------------------------------------------\\n\")\n",
    "\n",
    "# - Calculate a confidence interval for the mean using scipy.stats.sem() and stats.t.interval().\n",
    "\n",
    "n = len(before_scores)\n",
    "n = len(after_scores)\n",
    "\n",
    "mean = np.mean(before_scores)\n",
    "mean = np.mean(after_scores)\n",
    "\n",
    "sem = stats.sem(before_scores)\n",
    "sem = stats.sem(after_scores)  # sem = standard deviation / sqrt(n)\n",
    "\n",
    "confidence = 0.95\n",
    "\n",
    "confidence_interval = stats.t.interval(confidence, df=n-1, loc=mean, scale=sem)\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"95% Confidence Interval:\", confidence_interval)\n",
    "\n",
    "# - Set a significance level (a = 0.05) and determine if the null hypothesis should be rejected.\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference in scores before and after the training program.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference in scores before and after the training program.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda87321",
   "metadata": {},
   "source": [
    "# Q.4 - In markdown, describe:\n",
    "# - Type I Error (False Positive)\n",
    "# - Type II Error (False Negative)\n",
    "\n",
    "| Decision ↓ / Reality → | H₀ True        | H₀ False         |\n",
    "| ---------------------- | -------------- | ---------------- |\n",
    "| **Reject H₀**          | Type I Error ❌ | ✅ Correct        |\n",
    "| **Fail to Reject H₀**  | ✅ Correct      | Type II Error ⚠️ |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
